{"/prompts.docs/projects/vscode/":{"data":{"background#Background":"BackgroundWe worked on a VSCode extension to add a new Docker Language Service. The language sevice is IDE agnostic and can be attached to IDEs like Intellij, and neovim.\nThe current extension releases are available internally to Docker employees."},"title":"vscode"},"/prompts.docs/tools/":{"data":{"":"","ai-tools-for-dev#AI Tools for Dev":"","model-context-protocol#Model Context Protocol":""},"title":"Tools For Devs"},"/prompts.docs/tools/docs/authoring-prompts/":{"data":{"models#Models":"If you don’t specify a model in the preamble, the default model is currently set to be gpt-4 on openai. You can refer to other models and other endpoints using model and url.\n--- tools: - name: curl model: claude-3-5-sonnet-20241022 --- # prompt Run the curl command, in silent mode, to fetch gists for user slimslenderslacks from GitHub. If you specify an anthropic model, such as model: claude-3-5-sonnet-20241022, you do not need to specify the url. The correct anthropic endpoint will be used.\nℹ️ When using anthropic, do not change tool definitions to use json_schema. Use the openai standard of parameters - this will be automatically converted before making the api call. OpenAI-compatible endpoints (Ollama) You can specify other openai-compatible endpoints by including a url.\n--- tools: - name: curl url: http://localhost/v1/chat/completions stream: false model: llama3.1 --- # prompt Run the curl command, in silent mode, to fetch gists for user slimslenderslacks from GitHub. ℹ️ Set streaming to false if you’re using Ollama for tool calling. Ollama does not currently stream tool calls. ","prompt-files#Prompt files":"Prompt filesA prompt is markdown content with a preamble describing tools available to the agent when executing this prompt.\nWe use h1 headers to delineate sections that should be exposed as prompts. When authoring this markdown, you can include non-prompt sections using headers that don’t start with the word “prompt”.\nHere’s a simple prompt that will use Docker official curl container to fetch gists from Github.\n--- tools: - name: curl --- # prompt Run the curl command, in silent mode, to fetch gists for user slimslenderslacks from GitHub. ","prompt-templates#Prompt Templates":"It is common for prompts to contain parameters that are either extracted from a user interaction or, in the case of RAG, are populated by some sort of retrieval process. Markdown prompts can also contain template parameters.\nFor example, the above curl example could be re-written as a template with a {{ user }} parameter.\n--- tools: - name: curl url: http://localhost/v1/chat/completions stream: false model: llama3.1 --- # prompt Run the curl command, in silent mode, to fetch gists for user {{ user }} from GitHub. Binding values during testing When running in VSCode, you can set values of the parameters in the markdown preamble.\n--- parameter-values: user: slimslenderslacks --- Extractors Extractors are container functions that can be used to extract values when the prompt has been deployed to a server. These extractors are also used to populate default values for a prompt when it is used from an MCP client.\nExtractor definitions also follow the pattern of compose services. They are just docker images but with the additional requirement that they should write application/json to stdout. This json will be used to populate the context for binding parameters in the prompt template.\n--- extractors: - name: linguist image: vonwig/go-linguist:latest command: - -json --- We can create lists if the extractor json output has array types. For example, if we run the linguist tool to extract language from a project, our prompt can list them using the following template. You need to be familar with the json format output by linguist (eg that it creates lists of maps with a language key).\n--- extractors: - name: linguist --- # prompt {{#linguist}} This project contains {{language}} code. {{/linguist}} Template Engine We support two templating engines today.\nmustache is the default django If you want to use django, then add the following field in the markdown preamble.\n--- prompt-format: \"django\" --- MCP arguments ","tools#Tools":"Some tools, like curl, are available by default. However, the container for a tool can also be defined inline.\n--- tools: - name: ffmpeg description: run the ffmpeg command parameters: type: object properties: args: description: arguments to pass to ffmpeg type: array items: type: string container: image: linuxserver/ffmpeg:version-7.1-cli command: - \"{{args|into}}\" --- # prompt Use ffmpeg to convert the file UsingPuppeteer.mp4 into an animated gif file at 1 frame per second. The output file should be named UsingPuppeteer.gif. The name, description, and container fields are mandatory, and you’ll typically also have a parameters field to descript the json schema of the parameters that the agent will extract from the conversation.\nname should uniquely identify the tool. description is important. Good descriptions help the agent understand the tool and how to use it. container sticks close to the format of a compose service definition but does fully implement it. Many of the most common top-level arguments are supported (image, command, volumes) You can interpolate into parameter properties into the container definition using strings with double curly braces. These substitutions also support filters, such as into, which spreads an array parameter. There are some common patterns for moving parameters into the container runtime that we support for making it easier to use standard images."},"title":"Authoring Prompts"},"/prompts.docs/tools/docs/claude-desktop/":{"data":{"":"Enable mcp_run in your claude_desktop_config.json file using the following snippet. See the quickstart for Claude Desktop Users for more details.\n{ \"mcpServers\": { \"mcp_run\": { \"command\": \"docker\", \"args\": [ \"run\", \"--rm\", \"-i\", \"--pull\", \"always\", \"-v\", \"/var/run/docker.sock:/var/run/docker.sock\", \"--mount\", \"type=volume,source=docker-prompts,target=/prompts\", \"vonwig/prompts:latest\", \"serve\", \"--mcp\", \"--register\", \"github:docker/labs-ai-tools-for-devs?path=prompts/examples/hello_world.md\" ] } } Notice in the above snippet that the server is loaded with one example prompt, which you can view in our public github repo. This will have already been exposed using this MCP server so when using Claude Desktop, you can type “use hello docker to greet me with a joke”.\nYou’ll see a prompt asking if you want to run the “hello world” tool locally."},"title":"Using Claude Desktop"}}